---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

# SCRAPPING YOUTUBE 

##Libraries used 
 
```{r}
#install.packages("httr2")
#install.packages("httr")
#install.packages("jsonlite")
#install.packages("tibble")
#install.packages("tidyr")
#install.packages("dotenv")

library(httr2)
library(httr)
library(jsonlite)
library(tibble)
library(tidyr)
library(dotenv)
```
To be able to scrap information from the YouTube API we have to have an API KEY, this is sensitive information that we can not share with everyone, the steps to get a YouTube API are the following: 

1. First create a Google account to access the Google API console and be able to request an API key. 
2. Second step is to create a project on the Google Developers Console by clicking on the project dropdown that is next to the Google cloud logo. 
3. After creating a new project, go to the left side bar and select "APIs & Services" and then on then clic on the "Enable APIS and Services" and search for the  "YouTube Data API v3", select it and enable it. 
4. After enabling the API, click on the "Create Credentials" button and choose the API key option.
5. Finally, once created, the API key will be displayed on the screen, ensure to copy the API key and keep it secure.

Once we have the API KEY, we can use it on R but as mentioned above when having a API KEY we should be careful to handle it due to several security and privacy concerns API keys are credentials to access specific resources, therefore we should avoid to give this access to malicious actors to manipulate or retrieve sensitive information. 

So, the next step is to create a ".env" file to keep sensitive information in here and the exposure of this sensitive information to unauthorized persons.

```{r}
load_dot_env(file = ".env") #this function is to load environment variables from a dotenv file, where the file is named .env
API_KEY <- Sys.getenv("API_KEY") #we retrieve the value associated with the environment variable called "API-KEY"
```

```{r}
channel_username <- "CNN" #we define a variable called "CNN" since this is the channel_username that we want to retrive information from. 

#created the API request URL
url_channel <- paste0('https://www.googleapis.com/youtube/v3/channels?part=contentDetails&forUsername=', channel_username, '&key=', API_KEY)
#make the API request URL 
response_channel <- GET(url_channel)
#Parsing API response, we use "text" to specify that we want this type of content to be extracted from the HTTP response.
channel_info <- fromJSON(content(response_channel, "text"))
str(channel_info)



#We see if on the "items" element there is the comment section  
content_details <- channel_info$items$contentDetails
# We can see a data frame containing information about related playlists.
related_playlists <- content_details$relatedPlaylists
str(related_playlists)
```

## Get the video IDs

```{r}
# Function to get video IDs from the "uploads" playlist
get_video_ids <- function() {
  url_channel <- paste0('https://www.googleapis.com/youtube/v3/channels?part=contentDetails&forUsername=', channel_username, '&key=', API_KEY)
  response_channel <- GET(url_channel)
  channel_info <- fromJSON(content(response_channel, "text"))

  # Extract the "uploads" playlist ID for CNN's channel
  uploads_playlist_id <- channel_info$items$contentDetails$relatedPlaylists$uploads

  # Construct the URL for retrieving videos in the "uploads" playlist
  url_videos <- paste0('https://www.googleapis.com/youtube/v3/playlistItems?part=contentDetails&playlistId=', uploads_playlist_id, '&maxResults=100&key=', API_KEY)

  # Make the API request to get the videos
  response_videos <- GET(url_videos)
  videos_info <- fromJSON(content(response_videos, "text"))

  # Extract the video IDs
  video_ids <- videos_info$items$contentDetails$videoId

  return(video_ids)
}

# Retrieve video IDs from the "uploads" playlist
video_ids <- get_video_ids()

# Function to get comments for a specific video
get_comments_for_video <- function(video_id) {
  # Construct the URL for retrieving comments for the video
  url_comments <- paste0('https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=', video_id, '&key=', API_KEY)

  # Make the API request to get the comments
  response_comments <- GET(url_comments)
  comments_info <- fromJSON(content(response_comments, "text"))

  # Print or process the comments data as needed
  cat("Comments for Video ID:", video_id, "\n")
  print(comments_info)
  cat("\n")
}

# Retrieve comments for each video in the playlist
comments <- for (video_id in video_ids) {
  get_comments_for_video(video_id)
}
```

```{r}
library(tidyr)
library(tibble)
library(dplyr)
snippet.topLevelComment.snippet.textDisplay

#Extract relevant information from comments_info
comments_df <- tibble(
  kind = comments_info$kind,
  etag = comments_info$etag,
  nextPageToken = comments_info$nextPageToken,
  totalResults = comments_info$pageInfo$totalResults,
  resultsPerPage = comments_info$pageInfo$resultsPerPage,
  items = comments_info$items
)
comments_df <- unnest(comments_df, cols = c(items), names_sep = "_")
comments_df <- unnest(comments_df, cols = c(items_snippet), names_sep = "_")
comments_df <- unnest(comments_df, cols = c(items_snippet_topLevelComment), names_sep = "_")
comments_df <- unnest(comments_df, cols = c(items_snippet_topLevelComment_snippet), names_sep = "_")
comments_df <- unnest(comments_df, cols = c(items_snippet_topLevelComment_snippet_authorChannelId), names_sep = "_")
snippet_columns <- grep("^snippet\\.topLevelComment\\.", colnames(comments_df), value = TRUE)

# Unnest the identified snippet columns
comments_df <- unnest_longer(comments_df, col = all_of(snippet_columns))
print(comments_df) #we have all the info on a df now. 
print(comments_df$items_snippet_topLevelComment_snippet_textOriginal) #print the comment column

#Selected the columns that will be used for the sentiment analysis 
comments_df <- comments_df |> select(items_snippet_topLevelComment_snippet_authorDisplayName, items_snippet_topLevelComment_snippet_textOriginal,kind) |>
  rename( #renamed columns for a better undestanding
    user = items_snippet_topLevelComment_snippet_authorDisplayName,
    comment = items_snippet_topLevelComment_snippet_textOriginal,
    kind = kind
  )
```

### Text_mining

```{r}
library(tidytext)
library(dplyr)

get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")

# joy 
nrc_joy <- get_sentiments("nrc") %>%  
  filter(sentiment == "joy")

# negative
nrc_negative <- get_sentiments("nrc") |> 
  filter(sentiment == "negative")

# fear
nrc_fear <- get_sentiments("nrc") |> filter(sentiment == "fear")

# anger
nrc_anger <- get_sentiments("nrc") |> 
  filter(sentiment == "anger")

# trust
nrc_trust <- get_sentiments("nrc") |> 
  filter(sentiment == "trust")

# sadness
nrc_sadness <- get_sentiments("nrc") |> 
  filter(sentiment == "sadness")

# unnesting the word

yt_word <- comments_df |>
  unnest_tokens(word, comment) |> 
  distinct(user,word)
```

##Sentiment Analysis 

```{r}
# trump_joy
yt_trump_joy <- yt_word |> 
    inner_join(nrc_joy) |> 
    count(word, sort = TRUE)

yt_trump_joy$type <- c("joy")

# trump_negative
yt_trump_negative <- yt_word |> 
  inner_join(nrc_negative) |> 
  count(word, sort = TRUE)

yt_trump_negative$type <- c("negative")

# trump_fear
yt_trump_fear <- yt_word |> 
  inner_join(nrc_fear) |> 
  count(word, sort = TRUE)

yt_trump_fear$type <- c("fear")

# trump_anger
yt_trump_anger <- yt_word |> 
  inner_join(nrc_anger) |> 
  count(word, sort = TRUE)

yt_trump_anger$type <- c("anger")

# trump_trust
yt_trump_trust <- yt_word |> 
  inner_join(nrc_trust) |> 
  count(word, sort = TRUE)
yt_trump_trust$type = c("trust")

# trump_saeness
yt_trump_sadness <- yt_word |> 
  inner_join(nrc_sadness) |> 
  count(word, sort = TRUE)

yt_trump_sadness$type <- c("sadness")

yt_trump <- rbind(yt_trump_joy, yt_trump_negative, yt_trump_negative, yt_trump_anger, yt_trump_trust, yt_trump_sadness)
```

### graph

```{r}
library(ggplot2)

unique(yt_trump$type)

yt_trump |> 
  group_by(type) |> 
  ungroup() |> 
ggplot(aes(word, n, fill = type)) +
  geom_col(show.legend = TRUE) +
  facet_wrap(~type,nrow = 3, scales = "free_x")

```
##Term frequency 
```{r}
yt_words <- yt_word |> 
  count(user, word, sort = TRUE) #we count word frequencies keeping a column for the user where the word comes from
yt_words
```

How many words for each user are on each comment? 

```{r}
total_words <- yt_words %>% 
  #group by user to sum all the totals in the n column of yt_words
  group_by(user) %>% 
  #created a column called total with the total of words by user
  summarize(total = sum(n))
total_words
```

```{r}
#we add a column with this total number to the dataframe bookwords
#we use left join because we need the join to keep all rows in book_words, regardless of repeating rows
words <- yt_words %>%
  left_join(total_words, by = "user")
  
words <- words |> 
  mutate(frequency = n / total)
words
```

```{r}
library(ggplot2)

#we calculate the distribution and put it in the x axis, filling by book
words |>  ggplot(aes(frequency, fill = user)) +
  geom_histogram(show.legend = TRUE) +
  facet_wrap(~user, ncol = 2, scales = "free_y")

```
SENTIMEN PLOT OF ALL COMMENTS ON YOUTUBE
```{r}
sentiments <- yt_trump %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  rename(count = n)
sentiments
```
```{r}
library(ggplot2)

sentiments %>%
  #we weight by the count column: a term and its sentiment associated multiplied by count
  count(sentiment, word, wt = count) %>%
  ungroup() %>%
  #we filter by words appearing more than 2 times in the comments
  filter(n >= 2) %>%
  #create a new column called n that is equal to the count, but with the sign flipped
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  #reorder for descending order of bars
  mutate(term = reorder(word, n)) %>%
  #plot settings
  ggplot(aes(n, term, fill = sentiment)) +
  geom_col() +
  labs(x = "Number of mentions (contribution to sentiment)", y = NULL)
```

As visualization above, it becomes evident that the lexicon associated with Trump leans more towards negative sentiments than positive ones. The prevalence of words expressing unfavorable sentiments surpasses those conveying a positive tone in discussions related to Trump on the YouTube comments for the CNN channel and playlist about "Politics". 



# SCRAPPING REDDIT

### Authentification

### Libraries that we have used

```{r}
rm(list = ls())
library(httr)
library(httr2)
library(jsonlite)
library(dplyr)
library(scrapex)
```

### OAuth 2.0

```{r}
#install.packages("dotenv")
library(dotenv)
dotenv::load_dot_env(file = ".env")
access_token = Sys.getenv("access_token")
```

### Requesting the data using API

```{r}
search_url <- "https://api.reddit.com/r/politics/comments"

search_params <- list(
  q = "Donald Trump",
  type = "comments",
  sort = "best",
  limit = 100,
  "Authorization" = paste("Bearer", access_token),
  "User-Agent" = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
)

# best
req <- GET(
  url = search_url,
  query = search_params
)

print(req)

status_code(req)

# Parse the JSON content from the response
response_json <- content(req, as = "text", encoding = "UTF-8")

# Convert the JSON content to a list
response_list <- fromJSON(response_json)

# Extract comments from the search results
link_comments <- data.frame(response_list$data$children$data$body)
print(link_comments)

link_comments <- link_comments %>%
  rename("text" = "response_list.data.children.data.body")

```

```{R}
# relevance
search_params_2 <- list(
  q = "Donald Trump",
  type = "comments",
  sort = "relevance",
  limit = 100,
  "Authorization" = paste("Bearer", access_token),
  "User-Agent" = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
)

req2 <- GET(
  url = search_url,
  query = search_params_2
)

print(req2)

# Parse the JSON content from the response
response_json_2 <- content(req2, as = "text", encoding = "UTF-8")

# Convert the JSON content to a list
response_list_2 <- fromJSON(response_json_2)

# Extract comments from the search results
link_comments_2 <- data.frame(response_list_2$data$children$data$body)
print(link_comments_2)

link_comments_2 <- link_comments_2 %>%
  rename("text" = "response_list_2.data.children.data.body")

```

```{R}
# top
search_params_3 <- list(
  q = "Donald Trump",
  type = "comments",
  sort = "top",
  limit = 100,
  "Authorization" = paste("Bearer", access_token),
  "User-Agent" = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
)

req3 <- GET(
  url = search_url,
  query = search_params_3
)

print(req3)

# Parse the JSON content from the response
response_json_3 <- content(req3, as = "text", encoding = "UTF-8")

# Convert the JSON content to a list
response_list_3 <- fromJSON(response_json_3)

# Extract comments from the search results
link_comments_3 <- data.frame(response_list_3$data$children$data$body)
print(link_comments_3)

link_comments_3 <- link_comments_3 %>%
  rename("text" = "response_list_3.data.children.data.body")
```

```{r}
# hot
search_params_4 <- list(
  q = "Donald Trump",
  type = "comments",
  sort = "hot",
  limit = 100,
  "Authorization" = paste("Bearer", access_token),
  "User-Agent" = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
)

req4 <- GET(
  url = search_url,
  query = search_params_4
)

print(req4)

# Parse the JSON content from the response
response_json_4 <- content(req4, as = "text", encoding = "UTF-8")

# Convert the JSON content to a list
response_list_4 <- fromJSON(response_json_4)

# Extract comments from the search results
link_comments_4 <- data.frame(response_list_4$data$children$data$body)
print(link_comments_4)

link_comments_4 <- link_comments_4 %>%
  rename("text" = "response_list_4.data.children.data.body")
```

```{r}
# new
search_params_5 <- list(
  q = "Donald Trump",
  type = "comments",
  sort = "new",
  limit = 100,
  "Authorization" = paste("Bearer", access_token),
  "User-Agent" = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
)

req5 <- GET(
  url = search_url,
  query = search_params_5
)

print(req5)

# Parse the JSON content from the response
response_json_5 <- content(req5, as = "text", encoding = "UTF-8")

# Convert the JSON content to a list
response_list_5 <- fromJSON(response_json_5)

# Extract comments from the search results
link_comments_5 <- data.frame(response_list_5$data$children$data$body)
print(link_comments_5)

link_comments_5 <- link_comments_5 %>%
  rename("text" = "response_list_5.data.children.data.body")
```


```{R}
trump = rbind(link_comments, link_comments_2, link_comments_3, link_comments_4, link_comments_5)

print(trump)
```

### Text_mining

```{r}
library(tidytext)

get_sentiments("afinn")
get_sentiments("bing")
nrc = get_sentiments("nrc")
```

### sentimental analysis
### nrc analysis (word count)

````{R}
# Using nrc first to segment the sentiments
# positive 
nrc_positive <- get_sentiments("nrc") %>% 
  filter(sentiment == "positive")

# negative
nrc_negative <- get_sentiments("nrc") |> 
  filter(sentiment == "negative")

# anger
nrc_anger <- get_sentiments("nrc") |> 
  filter(sentiment == "anger")

# trust
nrc_trust <- get_sentiments("nrc") |> 
  filter(sentiment == "trust")

# unnesting the word
word = trump |> unnest_tokens(word, text)
print(word)

```

```{R}
# trump_joy
trump_positive <- word |> 
    #we combine both lists, NRC and Emma's words
    inner_join(nrc_positive) %>%
    #we count the mentions of each word to find the most frequent
    count(word, sort = TRUE)

trump_positive$type <- c("positive")
print(trump_positive)

# trump_negative
trump_negative <- word |> 
  inner_join(nrc_negative) |> 
  count(word, sort = TRUE)

trump_negative$type <- c("negative")
print(trump_negative)

# trump_anger
trump_anger <- word |> 
  inner_join(nrc_anger) |> 
  count(word, sort = TRUE)

trump_anger$type <- c("anger")
print(trump_anger)

# trump_trust
trump_trust <- word |> 
  inner_join(nrc_trust) |> 
  count(word, sort = TRUE)

trump_trust$type = c("trust")
print(trump_trust)

trump_nrc = rbind(trump_positive, trump_negative, trump_anger, trump_trust)
print(trump_nrc)

# erase the word "vote" since it is neutral and duplicated a lot
trump_nrc = trump_nrc |> filter(word != "vote")
print(trump_nrc)
```

### graph for nrc analysis

```{r}
library(ggplot2)

sentiment_nrc <- trump_nrc |> filter(n > 11) |> 
ggplot(aes(word, n, fill = type)) +
  geom_col(show.legend = TRUE) +
  facet_wrap(~type, nrow = 4, scales = "free_x") +
  ylab(NULL) + 
  theme_minimal() +
  theme(strip.text = element_text(face = "bold"),
        axis.text.x = element_text(size = 5.8, face = 'bold'),
        plot.margin = unit(c(1, 1, 1, 0.1), "cm"),
        legend.position = "none")


print(sentiment_nrc)
```

### Term frequency analysis

```{r}
total_comment_words <- trump_nrc %>% 
  #we group by books to sum all the totals in the n column of book_words
  group_by(type) %>% 
  #we create a column called total with the total of words by book
  summarize(total = sum(n))

total_comment_words
```

```{r}
comment_words <- left_join(trump_nrc, total_comment_words)
comment_words
```
```{r}
comment_words <- comment_words %>%
  #we add a column for term_frequency in each novel
  mutate(term_frequency = n/total)

comment_words
```

```{r}
library(ggplot2)

#we calculate the distribution and put it in the x axis, filling by book
ggplot(comment_words, aes(term_frequency)) +
  #we create the bars histogram
  geom_histogram(show.legend = TRUE) +
  #we set the limit for the term frequency in the x axis
  xlim(NA, 0.03)
```

```{r}
ggplot(comment_words, aes(term_frequency, fill = type)) +
  #we create the bars histogram
  geom_histogram(show.legend = TRUE) +
  #we set the limit for the term frequency in the x axis
  xlim(NA, 0.04) +
  #plot settings
  facet_wrap(~type, ncol = 2, scales = "free_y")
```

```{r}
freq_by_rank <- comment_words %>% 
  group_by(type) %>% 
  #we create the column for the rank with row_number by book
  mutate(rank = row_number()) %>%
  ungroup()

freq_by_rank

freq_by_rank |> group_by(type) |> filter(rank == 1 | rank == 2)
```

```{r}
freq_by_rank %>% 
  ggplot(aes(rank, term_frequency, color = type)) + 
  #plot settings
  geom_line(linewidth = 1.1, alpha = 0.8, show.legend = TRUE) +
  theme_minimal()
```

