---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

# SCRAPPING YOUTUBE 

##Libraries used 
 
```{r}
# install.packages("httr2")
# install.packages("httr")
# install.packages("jsonlite")
# install.packages("tibble")
# install.packages("tidyr")
# install.packages("dotenv")

library(httr2)
library(httr)
library(jsonlite)
library(tibble)
library(tidyr)
library(dotenv)
```

```{r}
load_dot_env(file = ".env") #this function is to load environment variables from a dotenv file, where the file is named .env
API_KEY <- Sys.getenv("API_KEY") #we retrieve the value associated with the environment variable called "API-KEY"
```

```{r}
channel_username <- "CNN" #we define a variable called "CNN" since this is the channel_username that we want to retrive information from. 

#created the API request URL
url_channel <- paste0('https://www.googleapis.com/youtube/v3/channels?part=contentDetails&forUsername=', channel_username, '&key=', API_KEY)
#make the API request URL 
response_channel <- GET(url_channel)
#Parsing API response, we use "text" to specify that we want this type of content to be extracted from the HTTP response.
channel_info <- fromJSON(content(response_channel, "text"))
str(channel_info)



#We see if on the "items" element there is the comment section  
content_details <- channel_info$items$contentDetails
# We can see a data frame containing information about related playlists.
related_playlists <- content_details$relatedPlaylists
str(related_playlists)
```

## Get the video IDs

```{r}
# Function to get video IDs from the "uploads" playlist
get_video_ids <- function() {
  url_channel <- paste0('https://www.googleapis.com/youtube/v3/channels?part=contentDetails&forUsername=', channel_username, '&key=', API_KEY)
  response_channel <- GET(url_channel)
  channel_info <- fromJSON(content(response_channel, "text"))

  uploads_playlist_id <- channel_info$items$contentDetails$relatedPlaylists$uploads

  url_videos <- paste0('https://www.googleapis.com/youtube/v3/playlistItems?part=contentDetails&playlistId=', uploads_playlist_id, '&maxResults=100&key=', API_KEY)

  response_videos <- GET(url_videos)
  videos_info <- fromJSON(content(response_videos, "text"))

#Extract the video IDs
  video_ids <- videos_info$items$contentDetails$videoId

  return(video_ids)
}

# Retrieve video IDs from the "uploads" playlist
video_ids <- get_video_ids()
```

```{r}
# Function to get comments for a specific video
get_comments_for_video <- function(video_id) {
  url_comments <- paste0('https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=', video_id, '&key=', API_KEY)

  response_comments <- GET(url_comments)
  comments_info <- fromJSON(content(response_comments, "text"))

  cat("Comments for Video ID:", video_id, "\n")
  print(comments_info)
  cat("\n")
}

# Retrieve comments for each video in the playlist
comments <- for (video_id in video_ids) {
  get_comments_for_video(video_id)
}
```

```{r}
library(tidyr)
library(tibble)
library(dplyr)

#Extract relevant information from comments_info
comments_df <- tibble(
  kind = comments_info$kind,
  etag = comments_info$etag,
  nextPageToken = comments_info$nextPageToken,
  totalResults = comments_info$pageInfo$totalResults,
  resultsPerPage = comments_info$pageInfo$resultsPerPage,
  items = comments_info$items
)
comments_df <- unnest(comments_df, cols = c(items), names_sep = "_")
comments_df <- unnest(comments_df, cols = c(items_snippet), names_sep = "_")
comments_df <- unnest(comments_df, cols = c(items_snippet_topLevelComment), names_sep = "_")
comments_df <- unnest(comments_df, cols = c(items_snippet_topLevelComment_snippet), names_sep = "_")
comments_df <- unnest(comments_df, cols = c(items_snippet_topLevelComment_snippet_authorChannelId), names_sep = "_")
snippet_columns <- grep("^snippet\\.topLevelComment\\.", colnames(comments_df), value = TRUE)

# Unnest the identified snippet columns
comments_df <- unnest_longer(comments_df, col = all_of(snippet_columns))
print(comments_df) #we have all the info on a df now. 
print(comments_df$items_snippet_topLevelComment_snippet_textOriginal) #print the comment column

#Selected the columns that will be used for the sentiment analysis 
comments_df <- comments_df |> select(items_snippet_topLevelComment_snippet_authorDisplayName, items_snippet_topLevelComment_snippet_textOriginal,kind) |>
  rename( #renamed columns for a better undestanding
    user = items_snippet_topLevelComment_snippet_authorDisplayName,
    comment = items_snippet_topLevelComment_snippet_textOriginal,
    kind = kind
  )
```

### Text_mining

```{r}
library(tidytext)
library(dplyr)
```

```{r}
get_sentiments("nrc")
```

```{r}
# positive
nrc_positive <- get_sentiments("nrc") %>% 
  filter(sentiment == "positive")

# negative
nrc_negative <- get_sentiments("nrc") |> 
  filter(sentiment == "negative")

# anger
nrc_anger <- get_sentiments("nrc") |> 
  filter(sentiment == "anger")

# trust
nrc_trust <- get_sentiments("nrc") |> 
  filter(sentiment == "trust")
```

```{r}
# unnesting the word
yt_word <- comments_df |>
  unnest_tokens(word, comment) |> 
  distinct(user,word)
```

##Sentiment Analysis 

```{r}
# trump_positive
yt_trump_positive <- yt_word |> 
    inner_join(nrc_positive) |> 
    count(word, sort = TRUE)

yt_trump_positive$type <- c("positive")
print(yt_trump_positive)

# trump_negative
yt_trump_negative <- yt_word |> 
  inner_join(nrc_negative) |> 
  count(word, sort = TRUE)

yt_trump_negative$type <- c("negative")
print(yt_trump_negative)

# trump_anger
yt_trump_anger <- yt_word |> 
  inner_join(nrc_anger) |> 
  count(word, sort = TRUE)

yt_trump_anger$type <- c("anger")
print(yt_trump_anger)

# trump_trust
yt_trump_trust <- yt_word |> 
  inner_join(nrc_trust) |> 
  count(word, sort = TRUE)
yt_trump_trust$type = c("trust")
print(yt_trump_trust)

```

Merge all words with count

```{r}
yt_trump <- rbind(yt_trump_positive, yt_trump_negative,yt_trump_anger, yt_trump_trust)
```

Visualize the result
```{r}
library(ggplot2)

yt_trump_plot <- yt_trump |> 
  group_by(type) |> 
  ungroup() |> 
ggplot(aes(word, n, fill = type)) +
  geom_col(show.legend = TRUE) +
  facet_wrap(~type,nrow = 3, scales = "free_x")

print(yt_trump_plot)
```

##Term frequency analysis

```{r}
yt_words <- yt_word |> 
  count(user, word, sort = TRUE) #we count word frequencies keeping a column for the user where the word comes from
yt_words
```

```{r}
total_words <- yt_words %>% 
  group_by(user) %>% 
  summarize(total = sum(n))
total_words
```

```{r}
words <- yt_words %>%
  left_join(total_words, by = "user")
  
words <- words |> 
  mutate(frequency = n / total)
words
```

```{r}
library(ggplot2)
words |>  ggplot(aes(frequency, fill = user)) +
  geom_histogram(show.legend = TRUE) +
  facet_wrap(~user, ncol = 2, scales = "free_y")

```

Sentiment plot of all comments on YouTube CNN channel 

```{r}
sentiments <- yt_trump %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  rename(count = n)
sentiments
```

```{r}
library(ggplot2)

sentiments %>%
  #we weight by the count column: a term and its sentiment associated multiplied by count
  count(sentiment, word, wt = count) %>%
  ungroup() %>%
  #we filter by words appearing more than 2 times in the comments
  filter(n >= 2) %>%
  #create a new column called n that is equal to the count, but with the sign flipped
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  #reorder for descending order of bars
  mutate(term = reorder(word, n)) %>%
  #plot settings
  ggplot(aes(n, term, fill = sentiment)) +
  geom_col() +
  labs(x = "Number of mentions (contribution to sentiment)", y = NULL)
```

As visualization above, it becomes evident that the lexicon associated with Trump leans more towards negative sentiments than positive ones. The prevalence of words expressing unfavorable sentiments surpasses those conveying a positive tone in discussions related to Trump on the YouTube comments for the CNN channel and playlist about "Politics". 


# SCRAPPING REDDIT

### Authentification

### Libraries that we have used

```{r}
rm(list = ls())
library(httr)
library(httr2)
library(jsonlite)
library(dplyr)
library(scrapex)
```

### OAuth 2.0

```{r}
#install.packages("dotenv")
library(dotenv)
dotenv::load_dot_env(file = ".env")
access_token = Sys.getenv("access_token")
```

### Requesting the data using API

```{r}
search_url <- "https://api.reddit.com/r/politics/comments"

search_params <- list(
  q = "Donald Trump",
  type = "comments",
  sort = "best",
  limit = 100,
  "Authorization" = paste("Bearer", access_token),
  "User-Agent" = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
)


# best
req <- GET(
  url = search_url,
  query = search_params
)

print(req)

status_code(req)


# Parse the JSON content from the response
response_json <- content(req, as = "text", encoding = "UTF-8")

# Convert the JSON content to a list
response_list <- fromJSON(response_json)

# Extract comments from the search results
link_comments <- data.frame(response_list$data$children$data$body)
print(link_comments)

link_comments <- link_comments %>%
  rename("text" = "response_list.data.children.data.body")

```

### Text_mining

```{r}
library(tidytext)

get_sentiments("afinn")
get_sentiments("bing")
nrc = get_sentiments("nrc")

# joy 
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

# negative
nrc_negative <- get_sentiments("nrc") |> 
  filter(sentiment == "negative")

# fear
nrc_fear <- get_sentiments("nrc") |> 
  filter(sentiment == "fear")

# anger
nrc_anger <- get_sentiments("nrc") |> 
  filter(sentiment == "anger")

# trust
nrc_trust <- get_sentiments("nrc") |> 
  filter(sentiment == "trust")

# sadness
nrc_sadness <- get_sentiments("nrc") |> 
  filter(sentiment == "sadness")

# unnesting the word
word = title |> unnest_tokens(word, link_title) |> distinct(word)
```

### sentimental analysis

````{R}
# trump_joy
trump_joy <- word |> 
    #we combine both lists, NRC and Emma's words
    inner_join(nrc_joy) %>%
    #we count the mentions of each word to find the most frequent
    count(word, sort = TRUE)

trump_joy$type <- c("joy")

# trump_negative
trump_negative <- word |> 
  inner_join(nrc_negative) |> 
  count(word, sort = TRUE)

trump_negative$type <- c("negative")

# trump_fear
trump_fear <- word |> 
  inner_join(nrc_fear) |> 
  count(word, sort = TRUE)

trump_fear$type <- c("fear")

# trump_anger
trump_anger <- word |> 
  inner_join(nrc_anger) |> 
  count(word, sort = TRUE)

trump_anger$type <- c("anger")

# trump_trust
trump_trust <- word |> 
  inner_join(nrc_trust) |> 
  count(word, sort = TRUE)

trump_trust$type = c("trust")

# trump_saeness
trump_sadness <- word |> 
  inner_join(nrc_sadness) |> 
  count(word, sort = TRUE)

trump_sadness$type <- c("sadness")

trump = rbind(trump_joy, trump_negative, trump_negative, trump_anger, trump_trust, trump_sadness)
```

### graph

```{r}
library(ggplot2)

unique(trump$type)

trump |> group_by(type) |> head() |> ungroup() |> 
ggplot(aes(word, n, fill = type)) +
  geom_col(show.legend = TRUE) +
  facet_wrap(~type, nrow = 3, scales = "free_x")
```


