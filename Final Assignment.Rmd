---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

# SCRAPPING YOUTUBE 

##Libraries used 
 
```{r}
# install.packages("httr2")
# install.packages("httr")
# install.packages("jsonlite")
# install.packages("tibble")
# install.packages("tidyr")
# install.packages("dotenv")

library(httr2)
library(httr)
library(jsonlite)
library(tibble)
library(tidyr)
library(dotenv)
```

```{r}
load_dot_env(file = ".env") #this function is to load environment variables from a dotenv file, where the file is named .env
API_KEY <- Sys.getenv("API_KEY") #we retrieve the value associated with the environment variable called "API-KEY"
```

```{r}
channel_username <- "CNN" #we define a variable called "CNN" since this is the channel_username that we want to retrive information from. 

#created the API request URL
url_channel <- paste0('https://www.googleapis.com/youtube/v3/channels?part=contentDetails&forUsername=', channel_username, '&key=', API_KEY)
#make the API request URL 
response_channel <- GET(url_channel)
#Parsing API response, we use "text" to specify that we want this type of content to be extracted from the HTTP response.
channel_info <- fromJSON(content(response_channel, "text"))
str(channel_info)



#We see if on the "items" element there is the comment section  
content_details <- channel_info$items$contentDetails
# We can see a data frame containing information about related playlists.
related_playlists <- content_details$relatedPlaylists
str(related_playlists)
```

## Get the video IDs

```{r}
# Function to get video IDs from the "uploads" playlist
get_video_ids <- function() {
  url_channel <- paste0('https://www.googleapis.com/youtube/v3/channels?part=contentDetails&forUsername=', channel_username, '&key=', API_KEY)
  response_channel <- GET(url_channel)
  channel_info <- fromJSON(content(response_channel, "text"))

  uploads_playlist_id <- channel_info$items$contentDetails$relatedPlaylists$uploads

  url_videos <- paste0('https://www.googleapis.com/youtube/v3/playlistItems?part=contentDetails&playlistId=', uploads_playlist_id, '&maxResults=100&key=', API_KEY)

  response_videos <- GET(url_videos)
  videos_info <- fromJSON(content(response_videos, "text"))

  # Extract the video IDs
  video_ids <- videos_info$items$contentDetails$videoId

  return(video_ids)
}

# Retrieve video IDs from the "uploads" playlist
video_ids <- get_video_ids()
```

```{r}
library(tibble)
library(httr)
library(jsonlite)

get_comments_for_video <- function(video_id) {
  max_results <- 100
  keyword <- "Trump"
  next_page_token <- NULL
  all_comments <- character(0)

  repeat {
    url_comments <- paste0('https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=', video_id, '&maxResults=', max_results, '&key=', API_KEY)

    if (!is.null(next_page_token)) {
      url_comments <- paste0(url_comments, '&pageToken=', next_page_token)
    }

    response_comments <- GET(url_comments)
    comments_info <- fromJSON(content(response_comments, "text"))

    current_comments <- comments_info$items$snippet$topLevelComment$snippet$textDisplay

    filtered_comments <- grep(keyword, current_comments, ignore.case = TRUE, value = TRUE)

    all_comments <- c(all_comments, filtered_comments)

    if (is.null(comments_info$nextPageToken)) {
      break
    } else {
      next_page_token <- comments_info$nextPageToken
    }
  }

  # Return the list of filtered comments
  return(all_comments)
}

#Retrieve and filter comments for each video in the playlist
all_video_comments <- list()

for (video_id in video_ids) {
  comments_for_video <- get_comments_for_video(video_id)
  all_video_comments[[video_id]] <- comments_for_video
}

# Create a single tibble for all comments
comments_tibble <- tibble(comments = unlist(all_video_comments))

# Now, 'comments_tibble' contains a tibble with a column named 'comments'
print(comments_tibble)

```

### Text_mining

```{r}
library(tidytext)
library(dplyr)
```

```{r}
get_sentiments("nrc")
```

```{r}
# positive
nrc_positive <- get_sentiments("nrc") %>% 
  filter(sentiment == "positive")

# negative
nrc_negative <- get_sentiments("nrc") |> 
  filter(sentiment == "negative")

# anger
nrc_anger <- get_sentiments("nrc") |> 
  filter(sentiment == "anger")

# trust
nrc_trust <- get_sentiments("nrc") |> 
  filter(sentiment == "trust")
```

```{r}
stop_words
```

```{r}
#unnesting the words
yt_word <- comments_tibble |> 
  unnest_tokens(word, comments) |> 
  anti_join(stop_words) 
```

## Sentiment Analysis 

```{r}
# trump_positive
yt_trump_positive <- yt_word |> 
    inner_join(nrc_positive) |> 
    count(word, sort = TRUE)

yt_trump_positive$type <- c("positive")
print(yt_trump_positive)

# trump_negative
yt_trump_negative <- yt_word |> 
  inner_join(nrc_negative) |> 
  count(word, sort = TRUE)

yt_trump_negative$type <- c("negative")
print(yt_trump_negative)

# trump_anger
yt_trump_anger <- yt_word |> 
  inner_join(nrc_anger) |> 
  count(word, sort = TRUE)

yt_trump_anger$type <- c("anger")
print(yt_trump_anger)

# trump_trust
yt_trump_trust <- yt_word |> 
  inner_join(nrc_trust) |> 
  count(word, sort = TRUE)
yt_trump_trust$type = c("trust")
print(yt_trump_trust)
```

Merge all words with count

```{r}
yt_trump <- rbind(yt_trump_positive, yt_trump_negative,yt_trump_anger, yt_trump_trust)
```

## Visualize the result
```{r}
library(ggplot2)

yt_trump_plot <- yt_trump |> 
  group_by(type) |> 
  top_n(8, wt = n) |>  #selected the top 8 more repeated words for each sentiment 
  ggplot(aes(word, n, fill = type)) +
  geom_col(show.legend = TRUE) +
  facet_wrap(~type, nrow = 3, scales = "free_x") +
  theme(axis.text.x = element_text(size = 5.5))


print(yt_trump_plot)
```


## Term frequency 
```{r}
total_words <- yt_trump |> 
  group_by(type) |> 
  summarize(total = sum(n))

total_words
```

```{r}
words <- yt_trump |> 
  left_join(total_words, by = "type")
  
words <- words |> 
  mutate(frequency = n / total)

words
```

```{r}
library(ggplot2)

#we calculate the distribution and put it in the x axis, filling by type
ggplot(words, aes(frequency)) +
  #we create the bars histogram
  geom_histogram(show.legend = TRUE) +
  #we set the limit for the term frequency in the x axis
  xlim(NA, 0.015)
```

```{r}
ggplot(words, aes(frequency, fill = type)) +
  #we create the bars histogram
  geom_histogram(show.legend = TRUE) +
  #we set the limit for the term frequency in the x axis
  xlim(NA, 0.015) +
  #plot settings
  facet_wrap(~type, ncol = 2, scales = "free_y")
```

```{r}
freq_rank <- words %>% 
  group_by(type) %>% 
  #we create the column for the rank with row_number by type
  mutate(rank = row_number()) %>%
  ungroup()

freq_rank

freq_rank |> group_by(type) |> filter(rank == 1 | rank == 2)
```

```{r}
freq_rank %>% 
  ggplot(aes(rank, frequency, color = type)) + 
  #plot settings
  geom_line(linewidth = 1.1, alpha = 0.8, show.legend = TRUE) +
  theme_minimal()
```

Sentiment plot of all comments on YouTube CNN channel 

```{r}
sentiments <- yt_trump %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  rename(count = n)
sentiments
```

```{r}
library(ggplot2)

sentiments %>%
  #we weight by the count column: a term and its sentiment associated multiplied by count
  count(sentiment, word, wt = count) %>%
  ungroup() %>%
  #we filter by words appearing more than 90 times in the comments
  filter(n >= 90) %>%
  #create a new column called n that is equal to the count, but with the sign flipped
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  #reorder for descending order of bars
  mutate(term = reorder(word, n)) %>%
  #plot settings
  ggplot(aes(n, term, fill = sentiment)) +
  geom_col() +
  labs(x = "Number of mentions (contribution to sentiment)", y = NULL)
```

As visualization above, it becomes evident that the lexicon associated with Trump leans more towards negative sentiments than positive ones. The prevalence of words expressing unfavorable sentiments surpasses those conveying a positive tone in discussions related to Trump on the YouTube comments for the CNN channel and playlist about "Politics". 

# SCRAPPING REDDIT

### Authentification

### Libraries that we have used

```{r}
library(httr)
library(httr2)
library(jsonlite)
library(dplyr)
library(scrapex)
library(tidyr)
```

### OAuth 2.0

```{r}
#install.packages("dotenv")
library(dotenv)
dotenv::load_dot_env(file = ".env")
access_token = Sys.getenv("access_token")
```

### Requesting the data using API

```{r}
search_url <- "https://api.reddit.com/r/politics/comments"

search_params <- list(
  q = "Donald Trump",
  type = "comments",
  sort = "best",
  limit = 100,
  "Authorization" = paste("Bearer", access_token),
  "User-Agent" = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
)

# best
req <- GET(
  url = search_url,
  query = search_params
)

print(req)

status_code(req)

# Parse the JSON content from the response
response_json <- content(req, as = "text", encoding = "UTF-8")

# Convert the JSON content to a list
response_list <- fromJSON(response_json)

# Extract comments from the search results
link_comments <- data.frame(response_list$data$children$data$body)
print(link_comments)

link_comments <- link_comments %>%
  rename("text" = "response_list.data.children.data.body")

```

```{R}
# relevance
search_params_2 <- list(
  q = "Donald Trump",
  type = "comments",
  sort = "relevance",
  limit = 100,
  "Authorization" = paste("Bearer", access_token),
  "User-Agent" = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
)

req2 <- GET(
  url = search_url,
  query = search_params_2
)

print(req2)

# Parse the JSON content from the response
response_json_2 <- content(req2, as = "text", encoding = "UTF-8")

# Convert the JSON content to a list
response_list_2 <- fromJSON(response_json_2)

# Extract comments from the search results
link_comments_2 <- data.frame(response_list_2$data$children$data$body)
print(link_comments_2)

link_comments_2 <- link_comments_2 %>%
  rename("text" = "response_list_2.data.children.data.body")

```

```{R}
# top
search_params_3 <- list(
  q = "Donald Trump",
  type = "comments",
  sort = "top",
  limit = 100,
  "Authorization" = paste("Bearer", access_token),
  "User-Agent" = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
)

req3 <- GET(
  url = search_url,
  query = search_params_3
)

print(req3)

# Parse the JSON content from the response
response_json_3 <- content(req3, as = "text", encoding = "UTF-8")

# Convert the JSON content to a list
response_list_3 <- fromJSON(response_json_3)

# Extract comments from the search results
link_comments_3 <- data.frame(response_list_3$data$children$data$body)
print(link_comments_3)

link_comments_3 <- link_comments_3 %>%
  rename("text" = "response_list_3.data.children.data.body")
```

```{r}
# hot
search_params_4 <- list(
  q = "Donald Trump",
  type = "comments",
  sort = "hot",
  limit = 100,
  "Authorization" = paste("Bearer", access_token),
  "User-Agent" = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
)

req4 <- GET(
  url = search_url,
  query = search_params_4
)

print(req4)

# Parse the JSON content from the response
response_json_4 <- content(req4, as = "text", encoding = "UTF-8")

# Convert the JSON content to a list
response_list_4 <- fromJSON(response_json_4)

# Extract comments from the search results
link_comments_4 <- data.frame(response_list_4$data$children$data$body)
print(link_comments_4)

link_comments_4 <- link_comments_4 %>%
  rename("text" = "response_list_4.data.children.data.body")
```

```{r}
# new
search_params_5 <- list(
  q = "Donald Trump",
  type = "comments",
  sort = "new",
  limit = 100,
  "Authorization" = paste("Bearer", access_token),
  "User-Agent" = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
)

req5 <- GET(
  url = search_url,
  query = search_params_5
)

print(req5)

# Parse the JSON content from the response
response_json_5 <- content(req5, as = "text", encoding = "UTF-8")

# Convert the JSON content to a list
response_list_5 <- fromJSON(response_json_5)

# Extract comments from the search results
link_comments_5 <- data.frame(response_list_5$data$children$data$body)
print(link_comments_5)

link_comments_5 <- link_comments_5 %>%
  rename("text" = "response_list_5.data.children.data.body")
```


```{R}
trump = rbind(link_comments, link_comments_2, link_comments_3, link_comments_4, link_comments_5)

print(trump)
```

### Text_mining

```{r}
library(tidytext)
nrc = get_sentiments("nrc")
```

### sentimental analysis
### nrc analysis (word count)

````{R}
# Using nrc first to segment the sentiments
# positive 
nrc_positive <- get_sentiments("nrc") %>% 
  filter(sentiment == "positive")

# negative
nrc_negative <- get_sentiments("nrc") |> 
  filter(sentiment == "negative")

# anger
nrc_anger <- get_sentiments("nrc") |> 
  filter(sentiment == "anger")

# trust
nrc_trust <- get_sentiments("nrc") |> 
  filter(sentiment == "trust")
```

```{R}
# unnesting the word
word = trump |> unnest_tokens(word, text)
print(word)
```

```{R}
# trump_joy
trump_positive <- word |> 
    #we combine both lists, NRC and comments words
    inner_join(nrc_positive) %>%
    #we count the mentions of each word to find the most frequent
    count(word, sort = TRUE)

trump_positive$type <- c("positive")
print(trump_positive)

# trump_negative
trump_negative <- word |> 
  inner_join(nrc_negative) |> 
  count(word, sort = TRUE)

trump_negative$type <- c("negative")
print(trump_negative)

# trump_anger
trump_anger <- word |> 
  inner_join(nrc_anger) |> 
  count(word, sort = TRUE)

trump_anger$type <- c("anger")
print(trump_anger)

# trump_trust
trump_trust <- word |> 
  inner_join(nrc_trust) |> 
  count(word, sort = TRUE)

trump_trust$type = c("trust")
print(trump_trust)

trump_nrc = rbind(trump_positive, trump_negative, trump_anger, trump_trust)
print(trump_nrc)

# erase the word "vote" since it is neutral and duplicated a lot
trump_nrc = trump_nrc |> filter(word != "vote")
print(trump_nrc)
```

### graph for nrc analysis

```{r}
library(ggplot2)

sentiment_nrc <- trump_nrc |> filter(n > 11) |> 
ggplot(aes(word, n, fill = type)) +
  geom_col(show.legend = TRUE) +
  facet_wrap(~type, nrow = 4, scales = "free_x") +
  ylab(NULL) + 
  theme_minimal() +
  theme(strip.text = element_text(face = "bold"),
        axis.text.x = element_text(size = 5.8, face = 'bold'),
        plot.margin = unit(c(1, 1, 1, 0.1), "cm"),
        legend.position = "none")


print(sentiment_nrc)
```

### Term frequency analysis

```{r}
total_comment_words <- trump_nrc %>% 
  #we group by types to sum all the totals in the n column of comment_words
  group_by(type) %>% 
  #we create a column called total with the total of words by type
  summarize(total = sum(n))

total_comment_words
```

```{r}
comment_words <- left_join(trump_nrc, total_comment_words)
comment_words
```

```{r}
comment_words <- comment_words %>%
  #we add a column for term_frequency in each type
  mutate(term_frequency = n/total)

comment_words
```

```{r}
library(ggplot2)

#we calculate the distribution and put it in the x axis, filling by type
ggplot(comment_words, aes(term_frequency)) +
  #we create the bars histogram
  geom_histogram(show.legend = TRUE) +
  #we set the limit for the term frequency in the x axis
  xlim(NA, 0.03)
```

```{r}
ggplot(comment_words, aes(term_frequency, fill = type)) +
  #we create the bars histogram
  geom_histogram(show.legend = TRUE) +
  #we set the limit for the term frequency in the x axis
  xlim(NA, 0.04) +
  #plot settings
  facet_wrap(~type, ncol = 2, scales = "free_y")
```

```{r}
freq_by_rank <- comment_words %>% 
  group_by(type) %>% 
  #we create the column for the rank with row_number by type
  mutate(rank = row_number()) %>%
  ungroup()

freq_by_rank

freq_by_rank |> group_by(type) |> filter(rank == 1 | rank == 2)
```

```{r}
freq_by_rank %>% 
  ggplot(aes(rank, term_frequency, color = type)) + 
  #plot settings
  geom_line(linewidth = 1.1, alpha = 0.8, show.legend = TRUE) +
  theme_minimal()
```

## COMPARING YOUTUBE AND REDDIT COMMENTS RELATED TO TRUMP 
### Youtube
```{r}
words_yt <- words |> # YouTube
  mutate(platform = c("YouTube"))

words_yt <- as.data.frame(words_yt) 
words_yt <- words_yt |> rename("term_frequency" = "frequency")
words_yt <- words_yt |> filter(word != "vote")
```

### Reddit
```{r}
comment_words_rd <- comment_words |> # Reddit 
  mutate(platform = c("Reddit"))

comment_words_rd <- comment_words_rd |> filter(word != "influence")

combined_yt_rd <- rbind(words_yt, comment_words_rd)
combined_yt_rd |> group_by(platform) |>  arrange(desc(term_frequency))

```

### Visualization
```{r}
library(ggplot2)

combined_yt_rd |> 
  filter(type == "positive" | type == "negative") |> 
  filter(term_frequency > 0.016) |> 
  #reorder for descending order of bars
  mutate(term = reorder(word, term_frequency)) %>%
  #plot settings
  ggplot(aes(term_frequency, word, fill = type)) +
  geom_col() +
  labs(x = "Term frequencies (contribution to sentiment)", y = NULL) +
  facet_wrap(~platform, ncol = 2, scales = "free_y") +
  theme_minimal() +
  theme(strip.text = element_text(face = "bold"))
```

### Word cloud 
### Libraries needed for this plot 
```{r}
#install.packages("wordcloud2")
library(RColorBrewer)
library(wordcloud2)
```

### WordCloud2 - Youtube
```{r}
yt_wordclod <- combined_yt_rd |>
  filter(platform == "YouTube") |> 
wordcloud2(color = "random-dark",
            fontWeight = 550,
            size = 2,
            widgetsize = c(900, 500))

yt_wordclod
```

### WordCloud2 - Reddit
```{r}
Rd_wordcloud <- combined_yt_rd |>
  filter(platform == "Reddit") |> 
wordcloud2(color = "random-dark",
            fontWeight = 550,
            size = 0.8,
            widgetsize = c(900, 500))

Rd_wordcloud
```

## Interpretation

When we did the sentimental analysis, we were able to identify some words from people's opinion which are positive and negative related to Donald Trump. 
From the **YouTube**, there are more negative words (11704 in total) than positive words (10900 in total) related to comments about Donald Trump. 
There are positive words such as "love", "peace", "top", "freedom", "protect", "supreme" and "happy". 
On the other hand, negative words are "dictator", "criminal", "bad", "lie", "enemy", "traitor", and "prison". 

From the **Reddit**, there are more positive words (952 in total) than negative words (867 in total) in the comments related to Donald Trump. 
Positive words are "money", "special", "good", "pay", "completely", "president", and "primary". 
Negative words are "government", "shit", "bad", "problem", "propaganda", "wrong", and "case".

## Challenges

Our main challenges that we have had during data extraction using API were obtaining API_KEY and tokens and finding appropriate endpoint for searching and getting the right dataset. 
However, since Reddit and YouTube API has a well written API documentation, we were able to figure it out and get the comment data. 

