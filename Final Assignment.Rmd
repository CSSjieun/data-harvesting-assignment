---
title: "R Notebook"
output: html_notebook
---

# SCRAPPING YOUTUBE 

## LIBRARIES 
```{r}
library(httr2)
library(jsonlite)
library(tibble)
library(tidyr)
library(dotenv)
```

```{r}
load_dot_env(file = ".env")
API_KEY <- Sys.getenv("API_KEY")
print(API_KEY)
```

```{r}
channel_username <- "CNN"

# Get the list of uploads for the CNN channel
url_channel <- paste0('https://www.googleapis.com/youtube/v3/channels?part=contentDetails&forUsername=', channel_username, '&key=', API_KEY)
response_channel <- GET(url_channel)
channel_info <- fromJSON(content(response_channel, "text"))
str(channel_info)
content_details <- channel_info$items$contentDetails
str(content_details)
related_playlists <- content_details$relatedPlaylists
str(related_playlists)
```

## Get the video IDs

```{r}
# Function to get video IDs from the "uploads" playlist
get_video_ids <- function() {
  url_channel <- paste0('https://www.googleapis.com/youtube/v3/channels?part=contentDetails&forUsername=', channel_username, '&key=', API_KEY)
  response_channel <- GET(url_channel)
  channel_info <- fromJSON(content(response_channel, "text"))

  # Extract the "uploads" playlist ID for CNN's channel
  uploads_playlist_id <- channel_info$items$contentDetails$relatedPlaylists$uploads

  # Construct the URL for retrieving videos in the "uploads" playlist
  url_videos <- paste0('https://www.googleapis.com/youtube/v3/playlistItems?part=contentDetails&playlistId=', uploads_playlist_id, '&maxResults=5&key=', API_KEY)

  # Make the API request to get the videos
  response_videos <- GET(url_videos)
  videos_info <- fromJSON(content(response_videos, "text"))

  # Extract the video IDs
  video_ids <- videos_info$items$contentDetails$videoId

  return(video_ids)
}

# Retrieve video IDs from the "uploads" playlist
video_ids <- get_video_ids()

# Function to get comments for a specific video
get_comments_for_video <- function(video_id) {
  # Construct the URL for retrieving comments for the video
  url_comments <- paste0('https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=', video_id, '&key=', API_KEY)

  # Make the API request to get the comments
  response_comments <- GET(url_comments)
  comments_info <- fromJSON(content(response_comments, "text"))

  # Print or process the comments data as needed
  cat("Comments for Video ID:", video_id, "\n")
  print(comments_info)
  cat("\n")
}

# Retrieve comments for each video in the playlist
comments <- for (video_id in video_ids) {
  get_comments_for_video(video_id)
}


```

```{r}
library(tidyr)
library(tibble)

# Extract relevant information from comments_info
comments_df <- tibble(
  kind = comments_info$kind,
  etag = comments_info$etag,
  nextPageToken = comments_info$nextPageToken,
  totalResults = comments_info$pageInfo$totalResults,
  resultsPerPage = comments_info$pageInfo$resultsPerPage,
  items = comments_info$items
)
comments_df <- unnest(comments_df, cols = c(items), names_sep = "_")
comments_df <- unnest(comments_df, cols = c(items_snippet), names_sep = "_")
comments_df <- unnest(comments_df, cols = c(items_snippet_topLevelComment), names_sep = "_")
comments_df <- unnest(comments_df, cols = c(items_snippet_topLevelComment_snippet), names_sep = "_")
comments_df <- unnest(comments_df, cols = c(items_snippet_topLevelComment_snippet_authorChannelId), names_sep = "_")
snippet_columns <- grep("^snippet\\.topLevelComment\\.", colnames(comments_df), value = TRUE)

# Unnest the identified snippet columns
comments_df <- unnest_longer(comments_df, col = all_of(snippet_columns))
print(comments_df) #we have all the info on a df now. 
```

## Sentiment Analysis 








# SCRAPPING REDDIT

### Authentification

### Libraries that we have used

```{r}
rm(list = ls())
library(httr)
library(httr2)
library(jsonlite)
library(dplyr)
library(scrapex)
```

### OAuth 2.0

```{r}
install.packages("dotenv")
library(dotenv)
dotenv::load_dot_env(file = "data.env")
access_token = Sys.getenv("access_token")
```

### Requesting the data using API

```{r}
search_url <- "https://api.reddit.com/r/subreddit/search"
search_params <- list(
  q = "Donald Trump",
  sort = "relevance",
  limit = 100
)

req <- GET(
  url = search_url,
  add_headers("Authorization" = paste("Bearer", access_token)),
  add_headers("User-Agent" = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"),
  query = search_params
)

status_code(req)

# Parse the JSON content from the response
response_json <- content(req, as = "text", encoding = "UTF-8")

# Convert the JSON content to a list
response_list <- fromJSON(response_json)

# Extract link IDs from the search results
link_title <- response_list$data$children$data$title
print(link_title)

title <- data.frame(link_title)
print(title)
```

### Text_mining

```{r}
library(tidytext)

get_sentiments("afinn")
get_sentiments("bing")
nrc = get_sentiments("nrc")

# joy 
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

# negative
nrc_negative <- get_sentiments("nrc") |> 
  filter(sentiment == "negative")

# fear
nrc_fear <- get_sentiments("nrc") |> 
  filter(sentiment == "fear")

# anger
nrc_anger <- get_sentiments("nrc") |> 
  filter(sentiment == "anger")

# trust
nrc_trust <- get_sentiments("nrc") |> 
  filter(sentiment == "trust")

# sadness
nrc_sadness <- get_sentiments("nrc") |> 
  filter(sentiment == "sadness")

# unnesting the word
word = title |> unnest_tokens(word, link_title) |> distinct(word)
```

### sentimental analysis

````{R}
# trump_joy
trump_joy <- word |> 
    #we combine both lists, NRC and Emma's words
    inner_join(nrc_joy) %>%
    #we count the mentions of each word to find the most frequent
    count(word, sort = TRUE)

trump_joy$type <- c("joy")

# trump_negative
trump_negative <- word |> 
  inner_join(nrc_negative) |> 
  count(word, sort = TRUE)

trump_negative$type <- c("negative")

# trump_fear
trump_fear <- word |> 
  inner_join(nrc_fear) |> 
  count(word, sort = TRUE)

trump_fear$type <- c("fear")

# trump_anger
trump_anger <- word |> 
  inner_join(nrc_anger) |> 
  count(word, sort = TRUE)

trump_anger$type <- c("anger")

# trump_trust
trump_trust <- word |> 
  inner_join(nrc_trust) |> 
  count(word, sort = TRUE)

trump_trust$type = c("trust")

# trump_saeness
trump_sadness <- word |> 
  inner_join(nrc_sadness) |> 
  count(word, sort = TRUE)

trump_sadness$type <- c("sadness")

trump = rbind(trump_joy, trump_negative, trump_negative, trump_anger, trump_trust, trump_sadness)
```

### graph

```{r}
library(ggplot2)

unique(trump$type)

trump |> group_by(type) |> head() |> ungroup() |> 
ggplot(aes(word, n, fill = type)) +
  geom_col(show.legend = TRUE) +
  facet_wrap(~type, nrow = 3, scales = "free_x")
```


