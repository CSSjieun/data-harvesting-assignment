---
title: "Data harvesting assignment"
author: "Jieun Park"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Get the API from the Youtube

```{r}
install.packages("tuber")
library(tuber)
```

```{r}
yt_oauth(app_id = app_id,
         app_secret = app_secret,
         token = "")
```
```{r}
youtuber = data.frame(channel = "CNN",
                      channel_id = "UCupvZG-5ko_eiXAupbDfxWw")

youtuber_CNN = get_channel_stats(channel_id = "UCupvZG-5ko_eiXAupbDfxWw")
ytCNN_status = data.frame(channel = "CNN", youtuber_CNN$statistics)

library(ggplot2)
ggplot(ytCNN_status, aes(x = channel, fill = channel)) +
  geom_bar(aes(y = viewCount), stat = "identity") +
  geom_text(aes(label = paste0(format(viewCount, big.mark = ","), "View"),
                y = viewCount), stat = "identity", vjust = -0.5) +
  labs(title = "View Numbers") +
  xlab("") + ylab("") +
  theme(text = element_text(size = 15),
        panel.background = element_blank(),
        legend.position = "none",
        axis.ticks = element_blank(),
        axis.text.y = element_blank())
```

```{r}
install.packages("udpipe")
library(udpipe)

# Load pre-trained UDPipe model for POS tagging
model <- udpipe_download_model(language = "english")
ud_model <- udpipe_load_model(model$file_model)

# Define a function to perform POS tagging
perform_pos_tagging <- function(text) {
  # Tokenize the text
  tokens <- udpipe_annotate(ud_model, x = text)
  
  # Extract POS tags
  pos_tags <- as.data.frame(tokens)$upos
  
  return(pos_tags)
}
```

```{r}
library(dplyr)
cmt_CNN = get_all_comments(video_id = "IE5mdWcjD7Y")
cmt_CNN = data.frame(cmt_CNN) 
cmt_CNN = cmt_CNN |> mutate(likeCount = as.numeric(likeCount))
cmt_CNN$likeCount

cmt_CNN1 = cmt_CNN |> slice_max(likeCount, n=100)

cmt_CNN2 = cmt_CNN1 |> select(textOriginal, likeCount, id)
cmt_CNN2 = cmt_CNN2 |> mutate(likeCount = as.factor(likeCount))
cmt_CNN2 = tibble(cmt_CNN2)


cmt_pos_CNN = perform_pos_tagging(cmt_CNN2$textOriginal)
```

```{r}
sentence <- cmt_CNN2$textOriginal[1]
words <- unlist(strsplit(sentence, " "))
words


split_sentence_df <- function(data) {
  # Initialize an empty list to store the words for each sentence
  words_list <- list()
  
  # Loop through each row of the dataset
  for (i in 1:nrow(data)) {
    # Access the sentence from the textOriginal column
    sentence <- data$textOriginal[i]
    
    # Split the sentence into words
    words <- unlist(strsplit(sentence, " "))
    
    # Store the words for this sentence in the list
    words_list[[i]] <- words
  }
  
  # Convert the list of words to a data frame
  words_df <- data.frame(words = unlist(words_list), 
                         sentence_id = rep(1:nrow(data), sapply(words_list, length)))
  
  return(words_df)
}

# Apply the function to your dataset
words_df <- split_sentence_df(cmt_CNN2)

# Print the first few rows of the data frame
distinct(words_df)

stop_words

word =  words_df |> group_by(words) |> count(words, sort = TRUE) |> ungroup() |> 
  rename(word = words) |> 
  anti_join(stop_words)

nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

word |> 
    #we combine both lists, NRC and Emma's words
    inner_join(nrc_joy) %>%
    #we count the mentions of each word to find the most frequent
    count(word, sort = TRUE)

nrc_negative <- get_sentiments("nrc") %>% 
  filter(sentiment == "negative")

word |> 
    #we combine both lists, NRC and Emma's words
    inner_join(nrc_negative) %>%
    #we count the mentions of each word to find the most frequent
    count(word, sort = TRUE)

```

